La fin du travail : effets de manche des clercs qui sont du côté du manche et un discours pas clair au service du Capital.
Qualité de l’alimentation : une bataille politique et citoyenne prioritaire
Le partage des richesses produites par les Français : véritable enjeu de l’élection présidentielle 2017.
Faut-il se ressembler pour s’assembler ? (sociologie politique de La France Insoumise)
Comment expliquer la volonté des gouvernements d’affaiblir l’hôpital public en France ?
La répression policière et judiciaire à l’encontre de journalistes exerçant leur métier d’informer librement.
L’empreinte de Durkheim et de Weber au sein de la gauche française
Une réflexion de Patrice Cohen-Seat au chevet du Front de Gauche.
Des mêmes auteurs
Debonrivage
En ne votant pas Mélenchon en 2017 des millions de Français ont pris le risque et la responsabilité de faire sombrer leur pays dans la plus grande crise économique de son histoire.
Zemmour : un talentueux et sinistre bonimenteur au service du Capital.
Il avait voté Macron ! Et maintenant …
La magistrature en souffrance.
L’ économie capitaliste au service des 1 % les plus riches.
Des réseaux neuronaux à la main invisible du marché.
samedi 26 juin 2021.
La numérisation du monde.
Des réseaux neuronaux à la main invisible du marché.
Le connexion isme de l’ultralibéral Hayek déconnecté de la réalité. La numérisation du monde.
**
1er Article
Interview dePablo Jensen : « Transformer le monde en chiffres, c’est une opération très subjective » Source : Libération. par Erwan Cario, publié le 4 mai 2018
https://www.liberation.fr/debats/20...
Voyage au cœur de l’IA dossier Peut-on observer et modéliser la société humaine comme on manipule une expérience scientifique dans un laboratoire ? C’est la croyance sur laquelle s’appuie une bonne partie de l’économie mondiale et qui, pourtant, ne cesse de montrer ses limites, comme l’analyse le physicien dans son dernier essai. Les données, celles que nous produisons, celles qui nous évaluent, nous définissent-elles ? Les nombres en cascade qui orientent les politiques économiques reflètent-ils une quelconque réalité ? Ces questions, on a tendance à ne même plus se les poser car nous vivons entourés de ces indicateurs quantitatifs (le PIB, la confiance des ménages, les taux de rentabilité, etc.) censés aboutir à des décisions objectives, puisque les chiffres ne mentent jamais. Pablo Jensen, physicien et directeur de recherche au CNRS, s’est attaqué de front à ces croyances dans son essai Pourquoi la société ne se laisse pas mettre en équations (Seuil). Entretien en 2 pages, 10 382 caractères, 9 questions et 12 occurrences du mot « indicateur ».
D’où vient, historiquement, cette volonté de modéliser et de chiffrer la société ? C’est lié à la gestion d’un Etat centralisé qui veut connaître ce qui se passe dans son pays. Quand on parle du pouvoir absolu et divin des rois, il était dans les faits limité, car ce qu’ils connaissaient de leur territoire était très faible. Quand les Etats ont voulu en savoir plus sur leurs territoires, ils ont créé les statistiques. « Statistique » est aujourd’hui un mot mathématique neutre, mais son étymologie vient bien de « Etat ». Quand vous voulez connaître depuis un centre un grand nombre de lieux différents, vous êtes obligés de les homogénéiser un peu pour pouvoir les sommer et les agréger. Quand Napoléon, surtout en temps de guerre, a voulu savoir de combien de blé il pouvait disposer, de combien d’hommes, de combien de minerais, il a fallu standardiser pour pouvoir regrouper. Il y a donc tout un mouvement qui passe par les noms de famille, par les poids et mesures. Cette mise en chiffres est très politique dès le départ.
La mise en chiffres de la société arrive donc au même moment que la standardisation de normes physiques ? C’est la même tendance. Et pour gouverner tout un pays depuis un centre, ça suppose que vous soyez capable d’établir des communications. Ce phénomène s’accentue avec le train et le télégraphe. Ces mouvements de modernisation et de contrôle centralisé explosent dans la seconde moitié du XIXe siècle. Pour illustrer ça, on peut prendre l’exemple des orchestres. Pendant longtemps, chacun jouait dans sa ville, et il n’y avait pas besoin d’un la qui soit le même partout. Mais, une fois qu’on connecte les lieux, on est obligé d’uniformiser pour qu’un musicien qui voyage s’y retrouve. On a donc imposé le même la partout, la même température partout, etc. Il y a d’ailleurs un certain Albert Einstein qui a travaillé sur la possibilité de synchroniser les heures dans les différentes gares, et on pense que ça a abouti pour lui à la théorie de la relativité, mais c’est une autre histoire.
Et puis on a commencé à se servir d’indicateurs et de modèles mathématiques pour quantifier la société et valider des décisions… On pourrait aller chercher l’origine de leur utilisation de plus en plus massive dans l’influence des sciences dures, mais ce n’est pas vraiment le cas. Il y a une étude de l’historien des sciences Theodore Porter qui montre que cette pression pour utiliser des indicateurs quantitatifs apparaît quand la société ne fait plus confiance aux experts. Il prend l’exemple du Mississippi et de la construction des barrages dans les années 40. Jusque-là, les experts décidaient entre eux et on leur faisait confiance (en France, on a continué longtemps à leur faire confiance). Mais on a forcé ces experts à objectiver leurs décisions en quantifiant. Eux ne le voulaient pas, car ils savaient que c’était très difficile de mettre en nombres des choses comme la valeur socio-économique d’un barrage. Mais on leur a imposé cette transparence. C’est vrai qu’en objectivant, on simplifie, on perd de l’information, mais ça force à expliciter et il devient possible de contre-expertiser.
Dans l’exemple de Notre-Dame-des-Landes, on peut voir que le bureau d’étude qui a estimé le gain issu de la construction à 900 millions d’euros l’a fait sur des données très contestables. Du coup, si on a les moyens de se payer une contre-expertise, on peut avancer des arguments et il y a un débat. Alors que si c’est décidé par quatre experts dans un cabinet parce qu’ils sont censés être porteurs de l’intérêt général, on ne peut rien contredire. Ces indicateurs relèvent donc finalement d’une exigence démocratique.
L’utilisation de ces chiffres s’est généralisée et on a fini par leur attribuer une valeur d’objectivité. On ne contredit pas un chiffre… Ça vient peut-être de la manière dont les maths sont enseignées à l’école et du mysticisme qui les entoure. Tant qu’on reste à l’intérieur des maths, c’est un outil rigoureux et inattaquable. Si on énonce le théorème de Pythagore ou « 1 + 1 = 2 », personne ne le peut le contester. On comprend que ce savoir inattaquable puisse fasciner les gens. Imaginez qu’on ait le même type de savoir en politique et qu’on puisse déduire scientifiquement la meilleure solution à appliquer… Le but de mon livre, c’est de montrer que si les indicateurs quantitatifs ont certains mérites, ils ont aussi leur fragilité. Transformer le monde en chiffres, c’est une opération très subjective et ça nécessite des hypothèses. Et rien de ce que vous dites en utilisant des indicateurs n’est plus sûr que les hypothèses que vous avez faites au départ. Vous pouvez faire un tuyau aussi beau que vous voulez avec les mathématiques, si vous ne le connectez pas à une bonne source, vous n’aurez pas d’eau à l’autre bout.
Dans cette volonté de modéliser la société, pour aboutir à des prédictions, on arrive très vite à essentialiser l’individu, à en faire un atome social… C’est le côté épistémologique de ces modèles où on va remplacer l’humain par une sorte de petit robot, un automate qui va suivre les instructions qu’on lui a données. On constate à chaque fois que ça ne marche pas. Mais c’est important de souligner le côté politique : en gros, quand vous basez des modèles sur ça, et les modèles économiques le sont pour la plupart, avec des atomes sociaux qu’on définit par des préférences et une fonction d’utilité, ce sont, si on utilise l’image de l’anthropologue James Scott, des molécules dont le cerveau est ailleurs. Le cerveau, c’est le chercheur, qui a fixé ses préférences et qui voit se dérouler son monde. Et c’est excitant de créer un monde et de voir ce qu’il s’y passe. On joue à être un dieu. Mais, politiquement, ça nous ramène à cette idée de centralisation. Il y a des limites aussi bien scientifiques que politiques à ces modèles.
On est en train de voir apparaître des modèles conçus par des systèmes d’intelligence artificielle. Ces chiffres ne risquent-ils pas de démultiplier cet effet trompeur d’objectivité ? Ce sont vraiment des boîtes noires. A deux niveaux : celui de l’algorithme en tant que tel, et celui de l’efficacité. C’est très difficile de faire une étude objective pour savoir si ça marche ou pas, tout bêtement. Du coup, ça se prête au fantasme. Les quelques études qui ont été rendues publiques montrent que les systèmes de prédiction ne donnent rien. Il y a cet exemple de prédiction de la récidive aux Etats-Unis. Une étude a comparé les résultats des algorithmes à des décisions prises par des gens à partir des mêmes données (antécédents, casier, etc.). Et on obtient des résultats du même ordre, un peu au-dessus du hasard. On voit que ces algorithmes ne font pas mieux que l’intuition humaine. Ça rappelle les prévisions de croissance économique. D’un côté, on a des modèles d’une complexité folle et, de l’autre, le simple fait de dire que la croissance d’une année est égale à celle de l’année précédente. Le taux d’erreur constaté est équivalent pour les deux méthodes.
Vous expliquez que les indicateurs ne reflètent pas la société, mais ils peuvent être utiles pour observer son évolution dans le temps… On a généralement une interprétation un peu naïve des indicateurs, où on leur demande d’être ontologiques, au sens où ils doivent représenter la réalité. Comme les maths, ils sont avant tout des systèmes qui connectent. Pour illustrer ça, on peut regarder du côté de Newton. Pour faire ses calculs sur le système solaire, il a considéré qu’une planète était juste une masse. C’est une réduction extrême de l’objet planète et, pourtant, ça marche. Pourquoi ? Parce qu’il a trouvé des stabilités dans le système solaire, et cette masse lui permet de connecter ce que fait la planète 1 par rapport à la planète 2. Si la masse est deux fois plus forte, l’attraction sera deux fois plus forte. Il ne faut donc pas juger la masse en disant que ça ne raconte rien de la planète, elle ne fait que la connecter à d’autres objets.
Il faut juger un indicateur quantitatif à sa capacité de connecter, de manière plus ou moins fiable, différentes situations. Ça marche bien pour la masse et la température, par exemple. Pour le nombre de citations d’un chercheur, qui est utilisé dans son évaluation, ou le PIB, il n’y a pas grand-chose derrière en termes de stabilité qui permette une utilisation fiable. Ce sont des simplifications moins légitimes.
Donc, si on prend le PIB, ça peut éventuellement servir à comparer des pays, mais ça ne dit rien d’un pays en lui-même. Ça capture quelque chose de l’économie. Si vous avez un PIB par habitant, qui vaut dix fois plus dans un pays que dans un autre, ça veut bien dire quelque chose. Quand on regarde le travail effectué par l’économiste Thomas Piketty sur plusieurs siècles, en essayant de rendre homogènes les données pour pouvoir connecter la façon dont on vit maintenant avec celle dont on vivait du temps de Balzac, on peut aboutir à des constats intéressants, comme le fait qu’on est en train de se diriger vers une société de rentiers. Cette connexion rendue possible enrichit notre perception sur ce qui est en train de se passer aujourd’hui.
Face à un indicateur, faut-il toujours se poser la question du futur vers lequel il veut nous porter ? C’est effectivement la bonne question. Par exemple, si on s’accroche au PIB et qu’on veut l’augmenter, on va isoler les manières efficaces de le faire et ça nous mène vers un certain type de futur. Si on décide d’y inclure certaines données écologiques, ça peut changer la direction. Il ne faut donc pas se contenter de questionner les indicateurs parce qu’ils sont réducteurs, ce qui est vrai, mais se demander aussi qui les a faits et où ça nous mène.
** ** 2e article
Intelligence artificielle et grand capital, même combat, d’après Pablo Jensen
Source : Le monde. Le 10/06/2021
https://www.lemonde.fr/sciences/art...
Dans « Deep Earnings », le physicien français décèle un lien inattendu entre les réseaux de neurones connectés et le néolibéralisme. Et en vient à proposer une nouvelle organisation politique, exempte de leurs défauts.
Le livre. Voici un livre court et stimulant, qui repose qui plus est sur une belle surprise. Le physicien Pablo Jensen tire un fil inattendu pour relier l’une des innovations techniques majeures de ces dernières années à l’une des théories économiques ayant le plus imprimé sa marque dans nos sociétés. A savoir les réseaux de neurones artificiels, ou apprentissage profond, et le néolibéralisme. En une centaine de pages, l’auteur, après avoir défini ces deux concepts, expose des analogies fécondes entre eux. Et les trouvant tous les deux limités, il esquisse des pistes tant scientifiques que politiques pour en corriger les défauts.
A l’origine de ce rapprochement inattendu, il y a la sixième référence bibliographique d’un article paru en 1958 sous la plume de Frank Rosenblatt, décrivant le premier réseau de neurones artificiels, le Perceptron. La note pointe vers un livre de 1952 de psychologie théorique d’un futur Nobel d’économie, l’Autrichien Friedrich Hayek, considéré comme un des penseurs du néolibéralisme, qui met l’accent sur la concurrence et le marché.
Article réservé à nos abonnés Lire aussi Intelligence artificielle : du Perceptron au premier Macintosh, la préhistoire d’une révolution Quels rapports entre les deux ? En fait, Hayek cherchait à décrire l’émergence de nos sensations avec des lois physiques. Il voulait aussi comprendre comment un ordre peut émerger de tant de cerveaux différents. Et c’est assez proche de ce que réalise un algorithme d’intelligence artificielle, tel que le Perceptron ou ses successeurs : il se construit en ajustant ses millions de paramètres de manière à répondre parfaitement à une série de questions.
Ordre spontané Cela rejoint la vision que développe Hayek des sociétés et de l’économie. Les neurones sont les agents d’un marché. L’architecture neuronale correspond à l’organisation de ce marché. Les « stimuli » sur les neurones sont les prix. Finalement, sans fixer de règles générales individuelles, il émerge dans les deux cas une réponse ou un prix qui correspond aux objectifs généraux fixés. Un ordre spontané a surgi sans que personne puisse vraiment l’anticiper et, surtout, ne puisse l’expliciter.
Néanmoins, cela comporte quelques défauts. Par exemple, l’IA résout des questions complexes, mais n’aide pas toujours à comprendre pourquoi telle réponse est advenue, apportant finalement peu de connaissances sur le monde qu’elle décrit. Politiquement, le néolibéralisme apparaît « autoritaire » puisqu’il évacue la question centrale des objectifs généraux des marchés et soumet donc les individus à des lois qui leur échappent.
En conclusion, Pablo Jensen, qui avait déjà décrit les difficultés à mathématiser les individus dans Pourquoi la société ne se laisse pas mettre en équations (Seuil, 2018), ose proposer une organisation politique n’ayant pas les défauts du néolibéralisme ou des IA. Un soupçon de « planification » pour disposer de règles. Une pincée de « marché » pour se sortir de situations complexes. Un brin de « communs » pour des organisations aux objectifs débattus collectivement. Et beaucoup d’intelligence pour imbriquer et relier ces trois « outils » ensemble. Facile.
I (Il vous te 3.16% de cet article à lire. La suite est réservée aux abonnés.)
** 3e article Hayek : connexionnisme et théorie de la coordination
Posted onAuthorOlivia Chevalier Source : journaldeslibertes.fr https://journaldeslibertes.fr/artic...
1 – Connexionisme
Dans L’ordre sensoriel de 1952, Friedrich Hayek a développé une théorie connexionniste de l’esprit humain tout à fait en cohérence avec sa pensée économique. En fait, les propos contenus dans cet ouvrage sont un déploiement de recherches en psychologie que Hayek avait menées dans les années 1920, inspirées par les travaux de Ernst Mach, avant qu’il ne se tourne vers l’économie, sous l’influence de l’économiste Ludwig von Mises.
L’ordre sensoriel expose un connexionnisme très similaire à celui que développe Donald Hebb, que l’on considère comme le père du connexionnisme, dans L’organisation du comportement, en 1949. Concernant les résultats concrets inspirés par ces modèles, il faut citer aussi Frank Rosenblatt, avec son Perceptron, en 1958.
Or, depuis les années 1980, après une domination du modèle rival, le computationnalisme, c’est ce modèle connexionniste –plus précisément le néo-connexionnisme – qui rend le mieux compte de l’ensemble qui caractérise le fonctionnement de la pensée humaine.
Que nous dit ce modèle ? Grosso modo, que l’esprit fonctionne de la manière suivante : il s’agit d’un réseau de neurones, c’est-à-dire un système distribué et massivement parallèle. Cela signifie que dans un tel réseau il n’y a pas de « processeur central », et que son état global est fonction de l’interdépendance de ses composants. Comme le dit Hayek au sujet des perceptions sensibles :
« The whole order of sensory qualities can be described in terms of (or “consists of nothing but”) all relationships existing between them ».[1]
Je tenterai donc de décrire la relation qui existe entre la conception connexionniste hayekienne de l’esprit et sa conception de l’économie et de la société ; relation qui prend une importance particulière au vu de la pertinence de ce modèle dans les recherches cognitivistes actuelles et en Intelligence Artificielle, même si la conception hayekienne est nécessairement moins avancée que ces développements contemporains.
Plus précisément, j’envisagerai les conséquences du connexionnisme, que j’exposerai dans un premier temps, sur la pensée économique et sociale de Hayek que je présenterai respectivement en un second et troisième temps.
Nous verrons que la conception économique et sociale de Hayek se pense en termes de coordination, la coordination étant selon nous l’homologue sur le terrain de l’action humaine du connexionnisme sur le terrain cognitif.
Le modèle connexionniste Hayek accomplit le geste platonicien de La République en faisant un parallèle entre la structure du psychisme (de l’âme) et la structure sociale (la cité)[2]. Cette structure commune a la caractéristique d’être dynamique, et non statique, ce qui est très important, comme nous le verrons.
En quoi ce modèle consiste-t-il ?
1. 1. Description du modèle
Dans L’ordre sensoriel, Hayek développe donc une conception connexionniste du fonctionnement de l’esprit. Ce modèle des sciences cognitives offre une description du fonctionnement cérébral, dont la pensée est le résultat, par un réseau d’unités liées entre elles par des connexions dotées d’un poids, appelé poids synaptique, dont l’intensité est fonction de la force liant ces unités. Ces unités sont les analogues des neurones, et ce poids modélise les effets des synapses liant ces neurones entre eux. Par exemple, plus une liaison est utilisée (comme un chemin), plus son poids est élevé.
En entraînant ces réseaux, les expériences ont montré une capacité à simuler certaines compétences cognitives humaines, comme la reconnaissance de forme, la lecture ou la reconnaissance de structures grammaticales simples.
Ce modèle connexionniste est l’alternative au modèle qui a dominé les sciences cognitives jusqu’aux années 1980, à savoir le computationnalisme. Ce dernier a été conçu par Turing et a été repris par Jerry Fodor notamment. Selon cette conception computationnaliste, l’intelligence et le fonctionnement cérébral sont analogues à un ordinateur, c’est-à-dire à une manipulation de symboles obéissant à des règles données par un programme. Ce programme joue en quelque sorte le rôle de l’inné dans les théories du psychisme. Notons d’ores et déjà qu’il existe un lien entre programme, innéisme et planification.
A l’inverse, le modèle connexionniste exclut toute programmation en vue de laisser le réseau « apprendre » en l’entraînant, c’est-à-dire sans « centre ». Par exemple, le réseau est capable d’apprendre, à partir d’exemples, à associer l’infinitif d’un verbe (anglais) à son passé, qu’il soit régulier (love/loved) ou irrégulier (catch/caught). Après un nombre important de présentations d’exemples, le modèle conjugue sans erreurs, alors qu’on ne lui a pas donné de règles de conjugaison via un programme. La saisie implicite de règles abstraites, d’une part, et leur application correcte, de l’autre, ont donc « émergé », dynamiquement, de l’entraînement du réseau à partir de simples cas.
Mais on doit préciser que certains ajustements extérieurs, correspondant à des algorithmes, sont nécessaires pour que le réseau apprenne. Ces ajustements sont destinés à corriger les erreurs du système afin de l’orienter. La correction est obtenue en modifiant les forces des liaisons entre l’unité et le reste du système. Cependant, cela n’invalide pas le caractère acquis de la saisie / application (comme nous le voyons chez le petit enfant qui apprend à parler).
Aperçu sur les réseaux de neurones
Voici, de manière simplifiée, comment le réseau se présente.
Modèle	Unités d’entrée (uei) ê	Unités cachées (uci) ê	Unités de sortie (usi) ê Fonction	Réception information ê	Intermédiaires ê	Résultats du processus ê Réalité	Neurones sensitifs	Autres neurones	Neurones moteurs Voici maintenant, schématiquement, comment le réseau fonctionne. Une valeur d’activation, correspondant à un nombre et représentant un élément extérieur, est affectée à chaque unité d’entrée.
Ainsi, comme le disent J.L. McClelland, D.E. Rumelhart et G.E. Hinton dans un célèbre article[3], « chaque unité a une valeur d’activation, qui correspond en gros à la force de l’hypothèse selon laquelle ce que cette unité représente est présent dans l’input [entrée] perceptuel ». Cette valeur d’activation de l’unité ue1, par exemple, est ensuite transmise à chaque unité cachée ucjvoisine de ue1. En fonction de ces valeurs d’activation que reçoit chaque ucj de la part des ueivoisines, chaque ucj va calculer sa propre valeur d’activation. Ces valeurs correspondent à des signaux se propageant dans le réseau. Ensuite, ce signal détermine les valeurs d’activation des usi.
Le schéma le plus simple du réseau est le suivant.
(Voir les schémas sur le site avec le lien précédent) Le poids synaptique des segments, ou connexion, ue1® uc1, par exemple, correspond donc à une force représentée par une certaine valeur. Une fonction d’activation ( f )permet de transformer le signal d’entrée, s’il dépasse un certain seuil, en signal de sortie.
Soit une unité (ou neurone) à n entrées, considérées comme des grandeurs numériques, notées x1à xn. Ces entrées sont des valeurs correspondant à des stimuli extérieurs si on a affaire à une ue, ou à une sortie d’autres u, dans le cas d’une uc. L’unité (neurone) va traiter ces grandeurs à l’instar d’une règle de calcul, c’est-à-dire en associant aux n entrées une sortie y. Pour cela, grosso modo, l’unité effectue d’abord la somme des entrées (S), somme qu’on dit pondérée car elle doit prendre en compte le poids des connexions avant la transformation, puis, si cette somme est supérieure à un certain seuil, elle applique à cette valeur une fonction d’activation (f) pour obtenir une valeur finale y, la sortie. Un modèle de neurone formel est donc une fonction à n variables et à valeurs réelles[4].
Étant donné que tous les uei et uci calculent à peu près toutes la même fonction d’activation simple, les compétences intellectuelles doivent d’abord dépendre des configurations des poids entre les différentes unités, donc des interactions.
En réalité, notre modèle est trop simple et je dois préciser deux choses : (i) il y a plusieurs couches de uc, et (ii) l’activation peut aller dans les deux sens : en avant (ui uj).
Pour comprendre comment ces réseaux fonctionnent, les deux concepts importants sont les suivants : l’entraînement, qui conditionne l’apprentissage, et la rétropropagation, qui en est le moyen, en permettant d’ajuster les poids en fonction des usi souhaitées à l’aide d’un algorithme de rétropropagation :
(1) Présentation d’un motif d’entraînement au réseau.
(2) Comparaison de la sortie du réseau avec la sortie ciblée.
(3) Calcul de l’erreur en sortie de chacun des neurones du réseau.
(4) Calcul, pour chacun des neurones, de la valeur de sortie qui aurait été correcte.
(5) Définition de l’augmentation ou de la diminution nécessaire pour obtenir cette valeur (erreur locale).
(6) Ajustement du poids de chaque connexion vers l’erreur locale la plus faible.
(7) Attribution d’un blâme à tous les neurones précédents.
(8) Recommencer à partir de l’étape 4, sur les neurones précédents, en utilisant le blâme comme erreur.
L’apprentissage correspond donc à un renforcement des connexions. ** On le voit, parce qu’il exige que le comportement, qui correspond aux sorties du réseau, ne soit pas le fruit d’un programme, mais le résultat d’un rapport au monde extérieur, à ses stimuli, et aux interactions entre unités, le modèle cognitif développé par Hayek suppose un empirisme radicalement hostile à tout innéisme, et, plus généralement, contre ce qu’il considère comme le rationalisme. On trouvera dans divers textes de Hayek cette critique du rationalisme, « L’inné contre l’acquis, une fois de plus », en 1971 (in Nouveaux essais de philosophie, de science politique, d’économie et d’histoire des idées, 2008), ou « Des sortes de rationalisme », un texte d’une conférence énoncée en 1964 (in Essais de philosophie, de science politique et d’économie, 2007).
Cependant, pour Hayek, l’esprit n’est pas une tabula rasa. En effet, de la même manière que le réseau possède une configuration, qu’il a une structure, notre esprit suppose une structure pour exister et fonctionner. Or, l’existence et le fonctionnement de notre esprit renvoient respectivement à deux concepts sur lesquels certains des textes de Hayek se pencheront : l’émergence et la complexité.
1.2. Émergence et complexité
Voici des définitions.
(i) L’émergence : on qualifie un phénomène ou un comportement comme émergent s’il est le résultat de l’interaction d’unités, tout en étant irréductible à ces unités. C’est dire que le phénomène émergent possède une structure propre, caractérisée par des propriétés qu’on ne retrouve pas au niveau des unités, même si ce sont ces unités, dans leur interaction, qui ont rendu ce phénomène possible. Bref, ce sont les interactions entre unités qui produisent les propriétés du système au niveau macro qu’on ne retrouve pas au niveau micro, tout comme les compétences cognitives humaines doivent provenir des interactions entre les unités.
Schéma d’un phénomène émergent
(ii) La complexité : on qualifie un système comme complexe s’il s’agit d’un ensemble constitué d’un grand nombre d’éléments en interaction, dont il est impossible de prévoir le comportement, l’évolution, et sa rétroaction (l’influence de l’état du système sur son état futur). Un système complexe, soulignons-le, est caractérisé par le fait que les interactions sont locales, ce qui signifie qu’il y a peu, ou pas, d’organisation centrale.
Je renvoie au texte de Hayek qui porte sur cette question, « La théorie des phénomènes complexes », qu’il a écrit en 1964 comme contribution à un ouvrage en l’honneur de Karl Popper, texte que nous trouvons dans les Essais.
On voit bien le lien qui existe entre émergence et complexité : un phénomène émergent, étant irréductible aux éléments dont les interactions lui ont donné naissance, est toujours le produit d’un système complexe.
Quelques propriétés de ce type de phénomène se déduisent alors :
(i) irréductibilité aux conditions de son existence. : la pensée ne se prête pas à une analyse de type réductionniste. Ce qui vaudra pour la société qui en est l’analogue à l’échelle supérieure.
(ii) acentrage : il n’y a pas de processeur central, mais des interactions locales qui produisent un phénomène global. Je reprends ce terme à J. Petitot et P. Rosenthiel, tel qu’ils l’ont analysé dans un article de 1974, intitulé « Automate asocial et systèmes acentrés ».
(iii) imprévisibilité : aucun programme ne pourra simuler le comportement du système ni ses résultats (les comportements). Le système n’est pas calculable. A plusieurs reprises d’ailleurs, Hayek a fait allusion aux théorèmes d’incomplétude de Gödel.
« Irréductionnisme » (néologisme), acentrage et imprévisibilité, sont donc les trois propriétés structurales des systèmes produisant des phénomènes émergents. La pensée est un phénomène de ce type, et le coup de maître de Hayek consiste à prétendre que le fonctionnement de la société participe également de cette structure, à travers la notion clef et problématique de Hayek : l’ordre spontané.
**
Quelles sont les conséquences au niveau global, c’est-à-dire collectif, de cette modélisation de la société ? Et comment cela fonctionne-t-il au niveau local, c’est-à-dire individuel ? Je traiterai d’abord le niveau collectif.
Le processus social : niveau global
Repos Nous allons voir qu’en vertu des éléments que nous avons posés, la conception de la société, entendue comme analogue au fonctionnement de la pensée, donc analogue à celui d’un réseau connexionniste, va découler très naturellement.
Niveau collectif Quelles sont les conséquences de l’utilisation d’un modèle connexionniste lorsque l’on procède à un changement d’échelle ? Grosso modo, les individus (mais pas seulement, on peut considérer des groupes également) correspondent aux unités, le rapport (relations) entre les individus sont les connexions, et l’ordre qui résulte de cette interaction correspond à l’ordre collectif émergent, qui est doté d’une structure irréductible à celle qui régit le niveau individuel (des unités).
Voilà ce dont il s’agit : entre le niveau individuel, régi par l’instinct, les besoins, puis les intentions rationnelles des individus, et le niveau collectif, qui obéit à des règles qui émergent de l’interaction entre ces individus, il existe une sorte de discontinuité. En effet, en vertu de la propriété (iii), ce ne sont pas les mêmes règles qui régissent ces deux niveaux individuel et collectif. Ils ne possèdent donc pas la même structure. En somme, un phénomène qu’on peut qualifier de dynamique, par opposition à mécanique, s’est produit : un nouveau type d’ordre a résulté de l’interaction individuelle, et celui-ci n’était ni prévisible (ou calculable) pour les individus, ni, par conséquent, voulu par eux.
Le corrélat de ce constat est le suivant : aucune organisation collective ne peut être déterminée par l’intention d’un ou de plusieurs individus, à moins de créer des dysfonctionnements. Or, ceci justifie le scandaleux « laisser-aller » tant critiqué par les économistes interventionnistes. On peut se référer au fameux texte de Keynes, intitulé The end of laissez-faire, publié en 1926.
Une comparaison peut être faite avec les insectes sociaux, sans pour autant nier la spécificité de la société humaine. En effet, nier l’émergence d’un ordre collectif irréductible au niveau individuel reviendrait à demander à une ou deux abeilles de trouver l’ordre le plus efficace pour leur société. Or, on voit bien que l’existence de la ruche, la détermination de la surface hexagonale des alvéoles, ainsi que le mode de récupération du pollen et du nectar n’a rien à voir avec une « décision » provenant d’un centre appartenant au niveau inférieur.
On trouvera cette comparaison entre la société et la ruche chez celui qui inspire les positions des économistes libéraux et anti-étatistes, et qui est le véritable créateur du concept de main invisible, avant Adam Smith : Bernard de Mandeville dans La Fable des abeilles, en 1705. Le sous-titre est intéressant : Fable of the Bees : or, Private Vices, Public Benefits.
On pourra objecter que cette comparaison ne serait valable que si l’homme n’était régi, comme les insectes, que par l’instinct (qui peut être considéré comme un programme). Or, il est doté d’intention et de raison. En fait, Hayek répond à cette objection, en posant la rationalité limitée des individus, comme nous le verrons dans la partie suivante. Mais nous pouvons déjà poser que cette intentionnalité et cette rationalité, même limitée, augmentent nécessairement la complexité et, ainsi, l’imprévisibilité des systèmes sociaux humains. Ce point a pour conséquence que, même s’ils partagent des propriétés communes, ces systèmes sociaux humains sont irréductibles aux systèmes vivants, puisqu’ils sont plus « riches » (le nombre de variables croît, et l’échelle « symbolique » vient s’ajouter à l’échelle matérielle et biologique).
Tout cela est lourd de conséquences puisque cela signifie qu’il faut penser la société ou le collectif à l’aide du concept d’ordre spontané : c’est l’état le plus efficace de la société lorsque les entités constituantes sont en libre interaction. En effet, l’ordre spontané est la configuration qui émerge spontanément dans un ensemble, entendu comme résultat des comportements individuels de ses entités, et sans être imposé par des facteurs externes à ces entités, ni commandés par un centre, puisque les interactions sont locales.
2.2. Conséquences politiques repos
Que la société fonctionne ainsi signifie que l’état le plus efficace que représente l’ordre émergeant spontanément ne peut être le fruit d’une intention d’une des entités, ou même d’un groupe d’entités très intelligentes appartenant à l’ensemble (au système). Cela se déduit aisément des propriétés du système, puisque :
(i) il n’y a que des interactions locales acentrées
(ii) et qu’il faut toutes les interactions pour obtenir l’état global
Conséquences politiques : pas de centre (acentrage) ® pas de programme ® pas de prévisibilité concernant le niveau macro à partir du niveau micro ® pas de planification possible (les effets escomptés d’un plan ne sont jamais certains et se révèlent le plus souvent « pervers »).
On comprend dès lors que Hayek, à l’instar de son maître von Mises, se soit toujours opposé aux systèmes politiques qui prétendent pouvoir régir, c’est-à-dire programmer, aussi bien l’activité économique que l’organisation sociale. Von Mises avait écrit un ouvrage sur la question, intitulé Economic Calculation in the Socialist Commonwealth, en 1938. Cette impossibilité de planifier justifie la méfiance, et même l’hostilité, de von Mises vis-à-vis de la mathématisation de l’économie. Ou plutôt, puisque la société est trop complexe, alors aucun calcul (au sens de prévisibilité) n’est possible, et ainsi la planification est par définition vouée à l’échec. Voici ce qu’il dit :
Les équations de la mécanique peuvent servir à prévoir des événements futurs, parce que le physicien est à même d’établir, au moins de façon approximative, des relations empiriques constantes entre les grandeurs de nature physique. Si l’on introduit ces constantes dans les équations, il devient alors possible de les utiliser pour le calcul. (…) Mais cela n’est pas possible avec les équations économiques. Car nous ne connaissons, dans le domaine des échanges sociaux, aucune relation constante entre les grandeurs. Les seules grandeurs que nous parvenons à déterminer ne possèdent qu’une signification historique et dépourvue de généralité.
Même si nous connaissons les conditions présentes, il nous est impossible, sur la base de cette connaissance, de faire aucune prédiction d’ordre quantitatif concernant l’évolution future des estimations. C’est là précisément que réside l’erreur essentielle que commettent tous ceux qui prétendent substituer une économie « quantitative » à l’économie « qualitative ».
Le traitement quantitatif des problèmes économiques ne peut jamais consister qu’en une histoire économique, jamais en une théorie économique. Or, il n’existe pas d’histoire économique du futur.
(…) Donc, même si nous connaissons l’état actuel du marché et si nous sommes en possession de toutes les données nécessaires à la détermination quantitative de la situation présente du marché, y compris les estimations des consommateurs qui parviennent à s’y exprimer, nous ne pouvons cependant rien savoir au sujet des estimations futures de ces consommateurs.
(…) L’évolution progressive des choses vers une situation d’équilibre, que nous considérons ici et qui constitue l’objet de notre recherche, implique un changement progressif des données qui conditionnent ces estimations et, par conséquent, une modification de ces estimations elles-mêmes.
(…) Mais nous ne pouvons évaluer les estimations d’aujourd’hui que dans la mesure où elles s’expriment dans les prix d’aujourd’hui. Et cela signifie que nous pouvons arriver à déterminer la grandeur de la demande d’une marchandise par le prix qui s’en est formé aujourd’hui sur le marché. Mais nous ne savons pas du tout ce que deviendrait la demande pour un prix différent. Nous ne connaissons même pas la forme des courbes d’offre et de demande, mais uniquement la position d’un point qui correspond à l’intersection actuelle des deux courbes ou, plus exactement, à une intersection qui vient d’être actuelle. C’est là absolument tout ce que puisse nous fournir l’expérience. Nous ne pouvons en tirer aucune connaissance précise des données qui seraient nécessaires à la résolution de nos équations.
Il faut enfin noter une troisième considération : l’état d’équilibre que décrivent nos équations est un état imaginaire ; il représente une notion logique qui, bien qu’absolument indispensable, n’est cependant que purement hypothétique et ne correspond à rien de réel[5].
Dès lors, cet interventionnisme planificateur est faux et nuisible[6]. Faux puisqu’il ne vaut que dans un cadre mécaniste, et non complexe, comme c’est le cas de la société humaine. Pour reprendre la comparaison donnée par Hayek, un ingénieur peut planifier le comportement d’une machine car il en connaît toutes les parties. Mais aucun homme ou groupe d’hommes ne peut planifier l’économie d’une société, car les individus qui la composent ne sont pas des pièces, mais des agents conscients (agissant selon des motifs conscients ou pas, ce qui ajoute à la complexité).
Il est également nuisible, et dangereux, et ce pour deux raisons : (i) car, dans l’ordre économique et social, on empêche l’ordre le plus efficace d’émerger et (ii) dans l’ordre politique, la prétention d’un groupe d’hommes, même élus démocratiquement, à savoir comment la société fonctionne, donc à vouloir programmer le devenir de la société, fait tendre cette société vers le totalitarisme. Viendra un moment où, en effet, l’interventionnisme aura tellement perturbé la dynamique, qu’encore plus d’interventionnisme sera nécessaire. Bref, toute intervention étatique implique une intervention plus grande encore. Selon Mises et Hayek, c’est exactement ce qui s’est passé en Allemagne avec les nazis, et en URSS avec Staline, et ce qui explique leur attachement au libéralisme, contre Keynes, attachement si coupable, et même diabolique, auprès de tant d’économistes. Sur ce point, un des disciples de Mises, Murray Rothbard, a eu une formule heureuse au sujet d’un autre anti-étatiste, à savoir Karl Marx[7] : « There is one good thing about Marx : he was not a Keynesian ».
Mais puisque cet ordre provient des comportements individuels, que doivent être ces individus, et quel type d’interaction doivent-ils entretenir pour produire, sans le savoir, l’état le plus efficace ?
** 3. La coordination : comment cela fonctionne-t-il ? Conséquence économique.
Les interactions individuelles produisant un ordre se pensent en termes de coordination, c’est-à-dire d’une harmonie des actions possédant une efficacité. Or, la coordination suppose un ajustement. Comment cet ajustement entre individus s’opère-t-il ? Pour répondre à cette question, nous devons d’abord justifier l’impossibilité, au niveau individuel, de calculer un type d’organisation[8] collective efficace.
3.1. La rationalité limitée
Les informations, les connaissances ou les significations que possèdent les individus ne sont pas localisées dans le système, mais distribuées dans le réseau, de telle sorte que l’idée dont nous sommes conscients correspond à « une configuration complexe d’activité distribuée sur plusieurs nœuds [connexions], chaque nœud participant à plusieurs configurations ». Je cite l’article fondamental de P. Smolensky, « On the proper treatment of Connectionism », de 1987. Mais l’ensemble des informations que possède un individu est toujours local, donc partiel.
En effet, à la différence des classiques et surtout des néo-classiques, Hayek ne conçoit pas les agents économiques comme rationnels. La raison est la suivante : il suppose, ce qui ne semble pas farfelu, que les agents ne peuvent pas avoir accès à toute l’information nécessaire pour décider de manière rationnelle. On devine pourquoi. En effet, posséder toute l’information supposerait que l’on connaisse l’état global de la société. Or, la connaissance de l’organisation collective échappe aux individus. Par conséquent, tout individu possède une rationalité limitée[9].
D’une façon qui paraîtra paradoxale à bon nombre d’économistes, c’est pour cette raison que Hayek défend le marché comme institution efficace. En réalité, sa défense du marché, processus d’agencement social supérieur aux autres, se déduit de la rationalité limitée, c’est-à-dire, de l’imperfection de la circulation de l’information. Pourquoi ? Car, la circulation de ce signal qu’est l’information étant imparfaite, cela rend les décisions centrales inopérantes, d’où il résulte, ainsi que nous le verrons, que seules les décisions décentralisées sont efficaces. Le marché permet donc une gestion efficace de l’information. Chacun des acteurs détenant une information partielle, contribue dès lors à cet agencement. Mais quelle est la nature de cette information fournie par le marché ; information qui permet la coordination des individus ?
3.2. La coordination par le système des prix
Si on poursuit notre parallèle connexionniste, dans la société, entendu comme système complexe, les individus sont donc en interaction, et cette interaction fait de chaque individu une unité à la fois d’entrée, cachée, et de sortie. Mais qu’est-ce qui entre ? Des stimuli du monde extérieur. Dans un contexte social humain, nécessairement linguistique, ces stimuli prennent la forme d’informations significatives, c’est-à-dire ayant un sens pour l’action humaine. Voici comment Hayek pose le problème de la coordination, dans Economics and Knowledge, en 1937 :
« (…) comment l’interaction spontanée d’un grand nombre d’agents, chacun possédant seulement des fragments de connaissance, provoque un état des affaires dans lequel les prix correspondent aux coûts, etc., et qui pourrait seulement être obtenu par l’action délibérée d’un agent possédant la connaissance de l’ensemble des individus. »
Quelles sont donc ces informations qui guident l’action individuelle et, par voie de conséquence, coordonne les individus ? Hayek donne la réponse dans Prix et production, publié en 1931, qui réunit quatre conférences données à la London School of Economics. Ces conférences étaient destinées à s’opposer à l’ouvrage de Keynes, Théorie de la monnaie, publié l’année précédente.
Dans ce texte, Hayek montre que les prix du marché sont ce qui permet de corriger les erreurs d’anticipation des individus, erreurs liées à un signal erroné, corrélat de la circulation imparfaite de l’information. Les prix agissent donc comme les ajustements dans notre algorithme de retropropagation.
La 1e vertu des prix, donc, consiste à fournir une information déterminante aux individus. En effet, ils fonctionnent comme des indices où se concentre l’information pertinente et dispersée. Les prix permettent donc la convergence de l’information dispersée vers une information unique.
Les prix les plus significatifs, c’est-à-dire ceux qui ont le plus de sens pour l’action, sont : (i) le taux d’intérêt naturel, qui oriente l’action des entrepreneurs[10] ; (ii) le prix relatif des biens de consommation par rapport aux biens de production, qui oriente les préférences des individus concernant la consommation et l’épargne.
Or, si les prix sont de bons indicateurs de l’action, c’est parce que la concurrence, dont ils sont le résultat, permet l’ajustement des données. La concurrence est donc le mécanisme, endogène, rendant possible l’ajustement des informations. En effet, la concurrence correspond à un processus composé d’une suite d’étapes où les agents adaptent leurs décisions et projets suite aux erreurs commises. Il s’agit donc d’une procédure de coordination qui sera perturbée par les facteurs exogènes, comme les interventions (étatiques notamment). Ce point fait comprendre l’importance que Hayek, avec l’école autrichienne en général, accorde à l’entrepreneur. Celui-ci est un acteur fondamental d’ajustement du marché, ainsi que l’économiste Israel Kirzner le démontre dans toute son œuvre[11].
Mais que reflète cette information ? La structure du marché elle-même. C’est la 2d vertu des prix.
Les prix fonctionnent comme stimuli d’entrée pour les individus, fournissant une information guidant l’action, par analogie avec la valeur d’activation, l’individu produisant, après un calcul, un comportement, ou valeur de sortie. On admet ici que le signal se propage des ue aux us grâce à un traitement cognitif opéré par les uc. Le prix, reflétant la structure du marché, a donc apporté à l’individu une information au sujet du niveau collectif, ou macro, lui permettant d’agir. Or, ceci signifie que le système fonctionne bien en feedback, ou rétroaction. Ainsi, les défauts de coordination, dus à des mécanismes exogènes, la monnaie et le crédit, perturbent la coordination entre individus, mais ils peuvent être corrigés par deux mécanismes endogènes, à savoir le système des prix et la concurrence.
** Conclusion
Quelle est la valeur de ce modèle ? Est-il une analogie, voire une métaphore, ou bien est-il en mesure de garantir scientifiquement la théorie économique ? Et s’il s’avérait que ce modèle cognitif connexionniste soit vrai, alors cela rendrait-il la théorie économique hayekienne valide ?
Toute dictature étant le résultat de l’illusion d’une possible commensurabilité des intelligences individuelles avec l’intelligence collective, la théorie hayekienne constituerait un rempart contre toute forme de politique à tendance centralisatrice et planificatrice, se croyant apte à décider à la place des individus en leur épargnant la peine d’exercer leur rationalité, bref de réfléchir.
Cela dit, n’y a-t-il pas, comme l’avait déjà remarqué Mises, un grand absent dans ce modèle, un paramètre dont l’intégration soit infirmerait, soit enrichirait, en apportant une dimension (ou échelle) dynamique supplémentaire à ce modèle, à savoir l’inconscient individuel, tel que Freud l’a développé ? Certes l’école de psychologie à laquelle Hayek se rattachait était anti-freudienne, et pourtant, le fait de considérer que les individus et leur action ne sont pas prévisibles, bref que l’action individuelle n’est pas mécanisable, le rapproche de manière inattendue de la position de Freud.
**
Notes
[1] « Le domaine (ordre) entier des qualités sensibles peut être exhaustivement décrit en termes de (ou « ne consiste en rien d’autre que ») l’ensemble des relations existantes entre elles. » in The Sensory Order, The University of Chicago Press, Chicago, Illinois, 1952, p.18.
[2] Il va de soi que la comparaison s’arrête ici, puisque Platon incarne l’idée que l’ordre le meilleur, parfait, est celui voulu par le philosophe-roi.
[3] « Une nouvelle approche de la cognition : le connexionnisme », in Le Débat, 1987/5 – n°47, p. 45-64.
[4] G. Dreyfus et al., Réseaux de neurones, Eyrolles, 2002.
[5] « Les Équations de l’économie mathématique et le problème du calcul économique en régime socialiste »,Revue d’économie politique, Paris, Librairie du Recueil Sirey, 1938, pp. 1055-1062.
[6] Ce qui n’exclue pas, au contraire, l’existence de règles. Celles-ci sont nécessaires car elles ont pour fin de favoriser l’émergence d’un ordre spontané. Analogiquement, le langage fonctionne grâce à des règles qui permettent de faire « émerger » des langues.
[7] Anti-étatiste par principe, ou plutôt comme nécessité historique, à la fin de l’histoire. Évidemment, avant que l’État ne dépérisse, selon Marx, il faut en passer par une phase ultra-étatiste correspondant à la dictature du prolétariat. Notons au passage que le IIIe Reich a été le premier à rigoureusement mettre en pratique les préconisations d’ordre économique du Manifeste du parti communiste.
[8] Nous employons le terme « organisation » ici dans la mesure où il s’agirait d’un ordre conçu et planifié par les hommes, bref d’un ordre « non spontané ».
[9] Cette théorie de la rationalité limitée sera développée dans les travaux d’Herbert Simon, par exemple dans le chapitre 2 des Sciences de l’artificiel intitulé « La rationalité économique : artifice de l’adaptation » (1996, ch. 2, 1981).
[10] C’est l’économiste suédois Knut Wicksell qui formule la distinction entre le taux d’intérêt naturel et le taux d’intérêt monétaire, dans son ouvrage Interest and Prices, publié en 1898. Hayek lui aussi oppose ce taux d’intérêt naturel au taux d’intérêt du marché, ce dernier constituant un signal erroné parce qu’il est fixé arbitrairement par les banques.
[11] Par exemple, Concurrence et Esprit d’entreprise, Economica (2005). L’entrepreneur sait saisir les opportunités (une manière plus « profitable » d’utiliser telle ressource), et ne correspond pas nécessairement à l’innovateur schumpétérien. Pour le dire rapidement, cette ignorance des agents, ou connaissance très limitée, va de pair avec la concurrence, qui elle-même est inséparable de l’entrepreneur, celui-ci étant entendu dans un sens élargi. La solidarité entre rationalité limitée, concurrence et entreprenariat est ce sur quoi repose une coordination efficace. A ce titre, l’entrepreneur crée de l’information. **
Commentaires HD
C’est une vision erronée du fonctionnement cérébral.
Concevoir le cerveau comme un assemblage interconnecté de 100 milliards de neurones par 10 000 à 20 000 synapses chacun, complexité dont émergerait l’esprit, la pensée, est une conception naïve et simpliste. Quiconque a quelques connaissances sur le fonctionnement du cerveau ou en neuropsychologie sait qu’il existe des centres fonctionnels spécifiques. Prenons l’exemple du thalamus. Le thalamus est une structure symétrique qui fait partie du diencéphale. Il occupe une place centrale dans le cerveau et est relié à son homologue dans l’autre hémisphère. Il sert de relais,des influx sensoriels et sensitifs remontant vers les régions correspondantes ; des influx moteurs partant des noyaux moteurs corticaux et du cervelet . Pour rejoindre les régions motrices.
Le capteur sensoriel de la vision : l’œil, l’œil est relié par son nerf optique à l’aire visuelle corticale du cerveau. Cette aire est divisée en une vingtaine de parties dont certaines sont spécialisées : analyse de la couleur, des formes, du mouvement. On pourrait multiplier les exemples. De même, l’hypothalamus est le régulateur de la faim. On peut donc mettre en valeur une certaine centralité dans le fonctionnement cérébral. Mieux, il peut même exister une certaine planification. Prenons l’exemple de la planification motrice. La planification motrice est un processus cognitif et psychomoteur, permettant d’élaborer un mouvement volontaire et de l’organiser en séquences avant de l’exécuter . Pour ce faire, avant chaque mouvement, le cerveau établit un plan moteur composé d’images mentales qui s’enchaînent . Cela est possible parce qu’il s’agit d’un automatisme qui anticipe le résultat de chaque mouvement. Lors de l’étape suivante, le cerveau spécifie les paramètres du mouvement, c’est-à-dire les éléments spatio-temporels (direction, force, amplitude, vitesse) et visuo-spatiaux qui orienteront l’action1. La planification motrice exige une bonne intégration des informations sensorielles de l’environnement (tactiles, visuelles et auditives) et celles provenant de son corps (kinesthésiques, proprioceptives et vestibulaires) afin que le mouvement produit soit adapté à la situation. Rappelons que « l’information » ou signal circulant entre les neurones ne se réduit pas à l’influx nerveux constitué d’électrons mais à la mise en action d’une multitude de substances chimiques (notamment les neurotransmetteurs) et met en jeu tout le système hormonal. La modélisation des réseaux neuronaux humains par des réseaux de neurones artificiels revient, si l’on veut rester modeste, à modéliser un haricot vert par un clou. Mais on pourrait ajouter que la modélisation de Hayek, elle, ne vaut pas un clou ! On retrouve chez Hayek une sorte de tentative de réintroduire le biologisme en économie.
Passons maintenant au modèle économique. On retrouve la conception libérale naïve de la conception atomistique de l’individu. Considéré comme une donnée anhistorique, c’est-à-dire non construite historiquement tant sur le plan biologique que culturel. Il ne reprend pas la conception libérale classique de l’individu calculateur rationnel mais prête à celui-ci une dimension largement irrationnelle. Mais celle-ci résulte d’une simple insuffisance d’information sur la connaissance du système ce qui est particulièrement réducteur et utilitariste. L’individu est réduit à un agent économique échangeant de l’information, de l’énergie, de la matière (les marchandises) c’est-à-dire à un simple capital humain des par une force qui le dépasse : la main invisible du marché, nouvel Esprit Saint émergeant spontanément des interactions s’accomplissant au sein du marché. Mais cette fiction de Hayek ne s’arrête pas là. Toute l’histoire de l’humanité, depuis le mésolithique et l’apparition de la division sociale du travail a fait apparaître sur tous les continents des proto états, des micros états puis des états de plus en plus organisés ayant joué à des degrés divers, un rôle important dans l’activité économique des sociétés. Le développement du capitalisme depuis le XVIe siècle ou pour le moins depuis le XVIIIe siècle a fait apparaître une double contractualité pour les agents économiques : une contractualité centrale par rapport à l’État central et une contrac-tualité inter individuelle entre les opérateurs du marché. Dans un pays comme la France contemporaine, ces contractualités sont par exemple formalisées dans le code du commerce, le code général des impôts, le code de la consommation, etc. Hayek ne peut supporter la centralité, ni l’idée d’une quelconque planification, mais cette approche réductrice ne dit rien sur la centralité monopolistique de certaines grosses entreprises multinationales géantes (comme par exemple , de nos jours, les GAFAM) plus puissante que des états et sur l’incroyable concentration vitale mondiale entre les membres d’un nombre très réduit de milliardaires. La notion de rapports de classe ne fait évidemment pas partie de l’espace mental de Hayek ne comprend pas que ces « attracteurs étranges » que sont les grosses multinationales déforment par leur champ de force l’espace de la concurrence supposée libre et non faussée. Cette croyance en l’existence d’un marché auto régulé, tendant spontanément vers un équilibre harmonieux relève d’un pur fantasme qui ne s’appuie sur aucune réalité historique.
Par ailleurs, les grosses entreprises pratiquent une planification appelée « gestion prévisionnelle ». Quant à l’imprévisibilité notamment des comportements économiques et même politiques, le capitalisme numérique utilise l’ingénierie sociale pour les surveiller, les analyser les prévoir et les orienter.
** Annexe
À la découverte des automates cellulaires https://interstices.info/a-la-decou...
Les réseaux de neurones artificiels. Wikipédia https://fr.wikipedia.org/wiki/Cat%C...
D’autres articles sur l’intelligence artificielle sur notre site. http://www.gauchemip.org/spip.php?p...